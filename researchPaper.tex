\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.misc, decorations.pathmorphing, shapes.geometric, arrows.meta, positioning}


\title{A Distributed, Forward-Only Threshold-Gating Neural Architecture\\
with Hierarchical Memory, Temporal Plasticity, Blockchain Preservation,\\
and Incentivized Participation}
\author{
  \textbf{Kritivasas} \\
  \small Senior Software Developer at Buildops \\
  \small \texttt{(No University Affiliation)}
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We propose a distributed neural network architecture that eliminates backpropagation in favor of local, forward-only threshold-gating nodes, inspired by neurobiological principles of memory and adaptation. Each user device hosts a cluster of adaptive nodes, preserving privacy and resilience, and participates in a global network coordinated by blockchain for memory permanence and fair incentive distribution. The system features a three-level memory hierarchy, temporal plasticity via adaptive thresholds and timers, eligibility traces, and dynamic topology management via node/edge duplication and pruning. Our workflow analysis and experiments demonstrate robust, modular learning with long-term memory preservation, seamless scaling, and community-driven growth even in dynamic, unreliable environments.
\end{abstract}

\section{Introduction}
Modern deep neural networks have achieved impressive performance across domains, but rely heavily on centralized training, global error backpropagation, and large amounts of labeled data \cite{Rumelhart1986,LeCun2015}. This reliance creates challenges for scalability, distributed computation, privacy, and long-term robustness. In contrast, biological brains leverage highly modular, local learning mechanisms and adapt via event-driven, forward-only updates, supported by diverse forms of memory and sparse global signals.

Recent research has proposed biologically inspired alternatives to backpropagation, such as Hinton's forward-forward algorithm \cite{Hinton2022} and Hebbian or eligibility trace-based learning rules \cite{Hebb1949, Gerstner2018}, which emphasize local updates and temporally modulated plasticity. Inspired by these principles, we propose a \emph{forward-only, threshold-gating neural architecture} in which each node accumulates inputs and fires based on an adaptive threshold or temporal deadline, with all error and adaptation signals fed forward (never backpropagated).

Crucially, we extend this design into a fully distributed, privacy-preserving, and incentivized system:
\begin{itemize}
    \item Each user device hosts a cluster of adaptive threshold-gating nodes, preserving privacy by keeping raw data strictly local.
    \item Local adaptation is achieved using a three-level memory hierarchy (node, cluster, global), adaptive thresholds, eligibility traces, and event-driven plasticity.
    \item Memory, adaptation, and error signals are periodically consolidated, compressed, and uploaded as encrypted “memory capsules” to a global distributed store, with blockchain auditability for long-term permanence and transparent incentive accounting.
    \item Smart contracts allocate rewards based on each participant’s contribution, making the network robust to device dropout, dynamic scaling, and adversarial conditions.
\end{itemize}

By merging neural-level biological plausibility with robust distributed systems design, we present a framework for scalable, modular, and resilient AI that can learn, grow, and self-organize across billions of heterogeneous, privacy-preserving endpoints.

\section{Related Work}

\subsection{Biological and Local Learning Mechanisms}
A long line of research in neuroscience and computational modeling has explored learning rules that do not require global backpropagation. Hebb’s principle \cite{Hebb1949}, eligibility traces \cite{Gerstner2018}, and three-factor plasticity rules capture the role of local memory and neuromodulatory signals in biological brains. Such mechanisms allow local adaptation, temporally extended credit assignment, and robust learning across distributed, modular networks. Further, event-driven models and spiking neural networks (SNNs) \cite{Izhikevich2004, Pozzorini2013, Fusi2005} use threshold-based firing and adaptive temporal dynamics for efficient, biologically plausible computation. These models provide inspiration for our threshold-gating nodes, local memory buffers, and eligibility trace-based adaptation.

\subsection{Forward-Only Learning in Artificial Networks}
Alternatives to backpropagation for artificial neural networks have gained traction, particularly for their potential in scalable, asynchronous, and modular AI. Hinton’s forward-forward algorithm \cite{Hinton2022} dispenses with gradient chains by using layer-wise “goodness” comparisons between positive and negative samples. Related work on local learning rules, energy-based models, and contrastive Hebbian learning (see \cite{Bengio2015}) further explore local updates that do not require global gradient propagation. Our approach extends these ideas to include adaptive threshold gating, temporally co-adapted timers, and purely forward-propagated error signals.

\subsection{Distributed, Privacy-Preserving, and Incentivized AI}
Practical deployment of large-scale neural systems increasingly requires distributed, privacy-preserving architectures. Federated learning and privacy-first neural systems \cite{Kairouz2021} have introduced on-device learning and aggregation, but often still rely on some form of global synchronization or limited centralization. Blockchain-based systems for distributed AI \cite{Christidis2016} and distributed ledgers for long-term storage and auditable contribution tracking \cite{Zyskind2015} offer tools for resilience, permanence, and incentivization. Our system uniquely combines threshold-gating neural computation with on-device privacy, long-term memory preservation via blockchain, and a transparent incentive model for participant contributions.

\subsection{Multi-Modal and Hierarchical Memory Systems}
Multi-modal learning and hierarchical memory architectures have been explored in deep learning \cite{Ngiam2011, Ramachandram2017} and neuroscience \cite{Lisman2020, Sara2009}. Hierarchical systems leverage local and global memory to handle distributed, asynchronous, and multi-domain tasks. Our three-level memory structure (local node, cluster, global) is directly inspired by such architectures, supporting modular growth, specialization, and resilient distributed computation.

\section{Methods}

\subsection{Threshold Gating Node and Local Adaptation}
The core component of our architecture is the \emph{threshold gating node}. Each node maintains:
\begin{itemize}
    \item An accumulator that sums inbound signals since the last release,
    \item An adaptive firing threshold \(\delta\),
    \item A timer tracking the elapsed time since the last firing,
    \item A forced-release interval \(timeToRelease\), after which the node fires even if the threshold is not met,
    \item An eligibility trace encoding recent activity for credit assignment,
    \item An error input, which is a locally received forward-fed signal (not a global backpropagated gradient).
\end{itemize}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{architecture_diagrams/1fe38644-c569-4658-b345-f5d0ddced01b.png}
    \caption{
        Detailed schematic of a threshold gating node. The node sums incoming values in the accumulator; fires (and updates) when the accumulator crosses the adaptive threshold \(\delta\) or the timer \(T\) expires. The eligibility trace tracks recent activity for credit assignment. Error inputs are used to adjust adaptation rates. Local memory stores all internal parameters and history.
    }
    \label{fig:threshold-gating-node}
\end{figure}

The node fires when its accumulator exceeds \(\delta\) or when its timer reaches \(timeToRelease\). Upon firing:
\begin{align}
\Delta \delta =
\begin{cases}
+\,\eta_{\delta}, & \text{if fired by threshold}, \\
-\,\frac{1}{2}\eta_{\delta}, & \text{if forced by timer,}
\end{cases}
\end{align}
where \(\eta_{\delta}\) is the threshold adaptation rate.  
The adaptation rate itself is updated by the error input:
\begin{equation}
\eta_{\delta} \leftarrow \eta_{\delta} + \alpha \cdot \text{errorInput}
\end{equation}
where \(\alpha\) controls sensitivity to error. The timer \(timeToRelease\) is also co-adapted, typically with a separate rate \(\eta_{\text{timer}}\), allowing flexible specialization in temporal dynamics:
\[
\begin{aligned}
&\textbf{If fired by threshold:}\\
&\quad \delta \leftarrow \delta + \eta_{\delta},\quad
timeToRelease \leftarrow timeToRelease + \eta_{\text{timer}}.\\
\\
&\textbf{If fired by timer:}\\
&\quad \delta \leftarrow \delta - \tfrac{1}{2}\eta_{\delta},\quad
timeToRelease \leftarrow \max\bigl(1,\, timeToRelease - \tfrac{1}{2}\eta_{\text{timer}}\bigr).
\end{aligned}
\]

\subsection{Three-Level Memory Hierarchy}

Our architecture implements a hierarchical memory system that draws on neuroscience and distributed systems principles to enable both rapid, context-sensitive adaptation and robust, long-term knowledge retention:
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{architecture_diagrams/memory.png}
    \caption{
        Diagram of the three-level memory hierarchy in the distributed neural architecture. Each node maintains local memory with privacy-preserving semantic masking and tagging. Cluster memory aggregates and distinguishes between personal (private, encrypted) and behavioral (shared, reusable) memory capsules, applying both automated and user-guided masking. Capsules are uploaded to decentralized storage, indexed and auditable via blockchain, and rapidly retrievable by vector/tag-based search for context-aware adaptation and lifelong learning.
    }
    \label{fig:memory-hierarchy}
\end{figure}

\begin{itemize}
    \item \textbf{Local Node Memory:} Each node maintains a structured internal buffer containing recent activations, local error signals, adaptive thresholds, eligibility traces, timers, and salient activity tags. Inspired by synaptic tagging and capture~\cite{Frey1997}, nodes mark significant events and prioritize which experiences are retained for plasticity and future adaptation. Sensitive information in this buffer is automatically detected and semantically masked (e.g., names replaced by \texttt{[PERSON NAME]}), ensuring privacy at the source.

    \item \textbf{Local Cluster Memory:} Clusters—groups of nodes on a device or within a single modality—share a memory space that aggregates outputs, errors, and adaptation histories from their constituent nodes. These clusters coordinate modular specialization, generate context embeddings or semantic hash tags for each “memory capsule,” and determine which updates are retained, discarded, or prepared for longer-term storage. When preparing capsules, the system enforces a dual protocol: strictly private (personal) data is encrypted and access-restricted, while abstracted, general behavioral knowledge can be tagged for sharing and rapid reuse across the network.

    \item \textbf{Global Memory:} Periodically, capsules from clusters are consolidated, compressed, and uploaded to decentralized storage solutions (such as IPFS or Filecoin), with each memory capsule linked to a unique high-dimensional vector tag or semantic hash. All memory uploads, node packet events, and capsule versions are registered in a blockchain-backed ledger and indexed in a decentralized vector database controlled by smart contracts~\cite{Petrosyan2023, Zyskind2015}. This infrastructure enables any authorized node or cluster to efficiently retrieve relevant memories based on context, semantic similarity, or recentness—facilitating rapid replay, adaptation, and credit allocation throughout the distributed network.
\end{itemize}

This three-level hierarchy ensures that learning and memory are both context-sensitive and robust: local nodes and clusters adapt quickly and protect privacy, while global memory guarantees persistent, auditable, and searchable knowledge even as devices join, leave, or rejoin the network. The use of semantic masking, context-based tagging, and blockchain auditing enables lifelong learning, scalable collaboration, and secure, privacy-preserving distributed AI.

\subsection{Node Borrowing Protocol}

To maximize resource utilization and support robust, collaborative learning across a distributed network, our architecture implements a formal node borrowing protocol. This protocol enables devices or clusters to temporarily “borrow” nodes—complete with their adaptive memory—from remote peers, providing additional computation, redundancy, or expertise as needed.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{architecture_diagrams/node-borrowing.png}
    \caption{
        Flowchart illustrating the Node Borrowing Protocol in the distributed neural network. The diagram details the full process: search and discovery of eligible node packets, permission and privacy checks, credit/incentive accounting via smart contracts, secure transfer and instantiation, operational use, and final update or reconciliation, with all actions tracked and audited on the blockchain ledger.
    }
    \label{fig:node-borrowing-protocol}
\end{figure}


\begin{itemize}
    \item \textbf{Discovery and Request:} When a device or cluster needs to augment its capacity or expertise, it queries the blockchain ledger or decentralized storage for available node packets matching specific context tags or behavioral capabilities. Search is enabled via the semantic tag index stored on-chain.
    
    \item \textbf{Permission and Privacy:} Each node packet is annotated with a privacy tag (“personal” or “behavioral”) and access control metadata. Only behavioral (non-personal) node memories are eligible for cross-device borrowing, and every packet is cryptographically signed. Upon request, the borrowing device must verify the packet’s signature and access rights.
    
    \item \textbf{Credit and Incentive Accounting:} Borrowing nodes incurs a credit cost, which is recorded via smart contracts on the blockchain. This mechanism incentivizes participants to contribute computational and memory resources to the network, while ensuring fair compensation and auditability.
    
    \item \textbf{Transfer and Instantiation:} Once permission and payment are confirmed, the node packet (memory snapshot) is securely transmitted to the borrower. The borrowing device decrypts and instantiates the node locally, integrating it as a full participant in its cluster for the duration of the lease.
    
    \item \textbf{Reinstatement and Update:} Upon completion of the borrowing period, updates to the node’s memory are either returned to the original owner (for behavioral memories) or reconciled via global memory consensus mechanisms, maintaining the integrity and lineage of all borrowed nodes.

    \item \textbf{Snapshot Preservation and Respawning After Borrowing:}
    When a node is borrowed and instantiated on a new device or cluster, its memory and adaptation state will evolve in response to local data and tasks. To maintain a verifiable and replayable history, our protocol requires that upon completion of the borrowing period, a new snapshot of the node's updated state is created, signed, and written to the ledger or decentralized storage. This final snapshot serves as the canonical state for any subsequent respawning, borrowing, or auditing events, preserving the complete lineage and credit assignments associated with all node usage. In this way, the system ensures that any changes made during borrowing are fully recorded and that nodes can be reliably restored to their most recent valid state across the network.
\end{itemize}

This borrowing protocol supports elastic scaling, enables knowledge transfer and rapid recovery after device dropouts, and ensures that personal/private data is never shared without explicit consent. All actions—discovery, borrowing, instantiation, update, and credit assignment—are fully auditable and governed by the distributed ledger, providing transparency, trust, and robust privacy guarantees throughout the network.

\subsection{Forward-Only Error and Local Credit Assignment}

Traditional neural networks rely on centralized, synchronous backpropagation to distribute error gradients and assign credit for learning. In contrast, our distributed architecture employs a fully forward-only protocol for error propagation and credit assignment, enabling both biological plausibility and scalable, asynchronous operation.

\paragraph{Local Error Propagation:}
Each threshold-gating node receives a forward-fed error input, which may be computed from local task loss, aggregated cluster performance, or broadcast global modulatory signals. These error signals are strictly feedforward—never requiring global chain-rule gradients—and are used to drive adaptation at each node and cluster independently.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.53\textwidth]{architecture_diagrams/flowchart-credit-assignment.png}
    \caption{
        Flowchart of forward-only error propagation and local credit assignment. Input data is processed by threshold-gating nodes; eligibility traces are updated upon firing. Locally or globally computed errors are fed forward to nodes for adaptation. Significant events are tagged and stored as memory capsules, indexed on the blockchain for rapid retrieval and credit assignment.
    }
    \label{fig:forward-error-credit-flow}
\end{figure}

\paragraph{Eligibility Traces and Temporal Credit Assignment:}
Within each node, eligibility traces serve as temporally decaying records of recent activation—functioning as synaptic “tags”~\cite{Gerstner2018, Frey1997}. These traces ensure that delayed error feedback can still reinforce or attenuate the synaptic changes that contributed to an outcome. Each node’s adaptation rate (threshold or timer) is modulated by both the magnitude of the current error signal and the strength of the stored eligibility trace, enabling precise, event-driven credit assignment.

\paragraph{Tagging for Memory Update:}
Nodes and clusters generate context tags or semantic hashes for memory capsules created during significant learning events, such as large error corrections or impactful discoveries. These tags are registered in the blockchain-backed vector index, supporting rapid retrieval, lineage tracking, and efficient credit assignment for future adaptation.

\paragraph{Distributed Credit Assignment and Incentives:}
Every memory capsule’s contribution to network learning—measured by its effect on error reduction, novelty, or frequency of use—is logged both locally (within the cluster) and globally (on the blockchain). The global aggregator and incentive module transparently register each contribution, ensuring fair, auditable credit allocation as nodes are borrowed, returned, or replicated across the network.

\paragraph{Summary:}
This forward-only, eligibility-trace-based approach allows each node to adapt based on local and global feedback, enabling robust, asynchronous learning and incentive alignment in dynamic, privacy-preserving, and decentralized environments—eliminating the need for centralized backpropagation.

\subsection{Distributed, Privacy-Preserving, and Blockchain-Based Operation}

Our architecture is purpose-built for secure, scalable deployment across a global network of heterogeneous user devices, each acting as an adaptive and privacy-preserving cluster within the distributed system.

\paragraph{Device-Level Clusters:}
Each user device operates one or more clusters of threshold-gating nodes. All raw data—including user inputs, sensor readings, and contextual metadata—remains strictly on-device. Learning, adaptation, and error evaluation occur locally, with only high-level adaptation statistics and derived summaries leaving the device.

\paragraph{Memory Capsule Generation and Tagging:}
At regular intervals, or in response to significant learning events, each device (or cluster) compresses, encrypts, and semantically tags its relevant adaptation histories, eligibility traces, and local context into a “memory capsule.” Tagging is performed using high-dimensional vector embeddings, context hashes, or interpretable semantic labels, supporting rapid future retrieval and disambiguation. Sensitive fields are semantically masked before export, and users may review or customize additional masking as needed.

\paragraph{Distributed Storage and Blockchain Indexing:}
Generated memory capsules are uploaded to decentralized storage solutions (e.g., IPFS, Filecoin, Sia), ensuring redundancy and resistance to tampering or censorship. Each capsule’s semantic tag, cryptographic hash, and retrieval pointer are registered via smart contract on a public or consortium blockchain, providing global auditability, immutable attribution, and secure access control.

\paragraph{Blockchain-Based Incentives and Credit:}
The blockchain maintains an immutable, transparent log of all memory uploads, node events, and network topology changes (e.g., node splits, merges, capsule updates, borrowing events). Smart contracts automate the allocation of incentives—such as tokens, compute credits, or access privileges—based on measurable contributions, including memory utility, relevance, hosting uptime, and demonstrated error reduction. This incentivizes diverse, active participation while ensuring fair, trustless collaboration.

\paragraph{Rapid and Secure Long-Term Memory Access:}
When nodes or clusters require relevant long-term knowledge (for adaptation, error correction, or credit reconciliation), they query the blockchain-indexed vector database for capsules with semantically similar tags or context. Decryption and usage are permitted only for authorized nodes, guaranteeing privacy, security, and compliance with user-defined or protocol-level data-sharing policies.

\paragraph{Resilience, Dropout, and Rejoining:}
Blockchain registration and distributed storage ensure that memory capsules are redundantly archived and globally discoverable, even as individual devices join, disconnect, or migrate between clusters. Upon rejoining, a device can efficiently resynchronize its state by querying the global index and securely retrieving and decrypting the latest relevant capsules.

\paragraph{Summary:}
This privacy-first, blockchain-based protocol achieves robust, decentralized learning and memory—combining local computation, rapid context-driven recall, auditability, and incentive-aligned scaling for distributed AI systems.


\section{Neuroscience Foundations}

The design of our architecture is inspired by foundational discoveries in neuroscience regarding memory, adaptation, and distributed learning. The following core principles directly inform the structure and operation of our system:

\begin{itemize}
    \item \textbf{Hierarchical and Distributed Memory:} Biological memory operates at multiple nested timescales—working, episodic, and long-term—across distributed anatomical substrates. Local memory traces are encoded in individual neurons and synapses, while higher-level structures (such as cortical columns and hippocampal circuits) coordinate integration and consolidation~\cite{Lisman2020, Buzsaki2019}. Errors, surprises, and novelty are detected locally (e.g., hippocampus, cerebellum), then globally signaled by neuromodulators (dopamine, acetylcholine, norepinephrine), which tag salient events for prioritization and consolidation~\cite{Sara2009, Doya2000, Shohamy2008}.

    \item \textbf{Local Learning and Eligibility Traces:} Synaptic plasticity is controlled not just by the correlation of pre- and post-synaptic activity (Hebbian learning), but also by global modulatory signals (“three-factor learning rules”) that regulate when and where plasticity occurs~\cite{Gerstner2018, Yagishita2014, Frémaux2016}. These mechanisms are often formalized as eligibility traces: local, temporally decaying markers that allow delayed feedback or reinforcement signals to influence which synapses are strengthened or weakened. This supports credit assignment even when feedback is sparse or temporally separated from the original activity.

    \item \textbf{Temporal Processing, Adaptive Thresholds, and Homeostasis:} Neurons dynamically adjust their firing thresholds and integration windows in response to activity, facilitating adaptive homeostasis, history-dependent learning, and multi-timescale integration~\cite{Pozzorini2013, Fusi2005, Turrigiano2012}. Such mechanisms ensure both rapid sensitivity to new information and long-term stability in the face of changing environments or ongoing plasticity.

    \item \textbf{Structural Plasticity, Modularity, and Self-Organization:} Synaptic growth, pruning, and rewiring enable the formation, maintenance, and dissolution of local subnetworks (columns, clusters, assemblies)~\cite{Holtmaat2009, Perin2011, Sporns2016}. Modular and hierarchical organization, observed from the microcircuit to the system scale, supports both robust specialization and flexible reconfiguration, enabling learning that is both resilient and scalable.

    \item \textbf{Memory Tagging, Prioritization, and Replay:} The brain employs tagging and replay mechanisms to prioritize salient experiences for consolidation during offline periods (e.g., sleep), as observed in hippocampal sharp-wave ripples and cortex-wide replay events~\cite{Carr2011, Girardeau2014, McClelland1995}. These processes ensure that only a subset of events—those deemed relevant or novel—are retained for long-term plasticity.
\end{itemize}

These neuroscience principles underpin our system’s approach to local and global memory, adaptive learning, credit assignment, error signaling, structural reorganization, and prioritization of salient experiences for robust, distributed, and biologically plausible AI.

\section{System Architecture}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{architecture_diagrams/6f6cd4fd-81a1-4ae3-be8c-6fd34920efe3.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}
\subsection{Device-Level Forward-Only Neural Networks}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{architecture_diagrams/48db0a56-63a0-40ef-9322-787cc6e465d9.png}
    \caption{Architecture of the Node}
    \label{fig:enter-label}
\end{figure}
Each user device hosts a cluster of threshold-gating nodes:
\begin{itemize}
    \item \textbf{Threshold-Gating Node:} Accumulates local inputs; fires when adaptive threshold (\(\delta\)) or timer is reached. Maintains eligibility traces and node-local memory.
    \item \textbf{Local Forward-Only Error:} After firing, node receives forward-only error or modulatory signal, updating thresholds/timers with no global backpropagation.
    \item \textbf{Cluster Memory:} Shared buffer collects node outputs, computes local error, and coordinates adaptation.
    \item \textbf{Dynamic Topology:} Nodes and edges are split/duplicated if usage exceeds thresholds; pruned if underused. Node split increases specialization; edge duplication improves bandwidth. Parameters governing splitting, duplication, and pruning are adaptive and increase with network complexity.
\end{itemize}

\subsection{Privacy and Local Adaptation}
    % Insert PNG or TikZ diagram here.
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{architecture_diagrams/bbe6c593-355f-42f7-a0a1-79f3937b8efa.png}
        \caption{Detailed overview of Memory Management}
        \label{fig:overview-mem-management}
    \end{figure}
Raw user data remains strictly on-device. Only summary statistics, error signals, and encrypted “memory capsules” (learned parameters, eligibility traces, etc.) are shared or uploaded for global consolidation.

\section{System Architecture}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.76\linewidth]{architecture_diagrams/6f6cd4fd-81a1-4ae3-be8c-6fd34920efe3.png}
    \caption{
        High-level overview of the distributed, forward-only threshold-gating neural architecture. Each user device operates as a privacy-preserving, adaptive cluster that contributes to and synchronizes with a blockchain-backed, globally coordinated network.
    }
    \label{fig:high-level-architecture}
\end{figure}

\subsection{Device-Level Forward-Only Neural Networks}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.68\linewidth]{architecture_diagrams/48db0a56-63a0-40ef-9322-787cc6e465d9.png}
    \caption{
        Internal architecture of a threshold-gating node and its memory components. The node accumulates local inputs, updates eligibility traces and timers, and adapts its firing threshold (\(\delta\)) based on forward-fed error signals and activity tags.
    }
    \label{fig:node-internal-architecture}
\end{figure}

Each user device hosts a dynamically sized cluster of threshold-gating nodes, collectively implementing a fully forward-only learning system. The core architectural elements are:

\begin{itemize}
    \item \textbf{Threshold-Gating Node:} Each node accumulates local input signals and fires when its adaptive threshold (\(\delta\)) or timer is reached. Node-local memory maintains eligibility traces, timers, activity tags, synaptic weights, and statistics for both recent and salient events.

    \item \textbf{Forward-Only Error Propagation:} Upon firing, the node receives a strictly forward-fed error or modulatory signal—never requiring global backpropagation. The error signal may be locally computed (task loss), aggregated from the cluster, or broadcast by global modulatory nodes. Adaptation of the node’s threshold and timer is a function of the error, current eligibility trace, and activity history.

    \item \textbf{Cluster Memory and Local Adaptation:} Node outputs and adaptation statistics are shared within a cluster-wide buffer. This enables local error evaluation, modular specialization, dynamic memory tagging, and computation of “memory capsules” for potential long-term storage.

    \item \textbf{Dynamic Topology Management:} Network topology is elastic: nodes and edges are dynamically split or duplicated when their usage exceeds local or global thresholds, allowing for specialization and increased bandwidth. Conversely, underutilized nodes and edges are pruned to conserve resources. Splitting, duplication, and pruning thresholds increase adaptively as network complexity grows, preventing runaway expansion.

    \item \textbf{Node Borrowing and Packetization:} Clusters may borrow node packets (including memory, adaptation state, and activity tags) from other devices. The borrowing protocol ensures that only behavioral memory is shared (not personal/private), credit incentives are properly allocated, and post-borrowing snapshots are recorded and auditable on the blockchain. After borrowing, the node’s updated state is also snapshotted, cryptographically signed, and logged for audit and future respawning.
\end{itemize}

\subsection{Privacy and Local Adaptation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.63\linewidth]{architecture_diagrams/bbe6c593-355f-42f7-a0a1-79f3937b8efa.png}
    \caption{
        Detailed overview of hierarchical memory management, including local node memory, cluster memory capsules, personal/behavioral tagging, semantic masking, and blockchain-indexed global memory. All raw data remains on-device; only encrypted, context-tagged summaries are exported.
    }
    \label{fig:overview-mem-management}
\end{figure}

A core design principle is \textbf{privacy by default}. All raw user data (input, sensor streams, context) is processed strictly on-device. Only derived, summary statistics, error signals, and encrypted “memory capsules” are shared for network-wide learning or adaptation.

\begin{itemize}
    \item \textbf{Semantic Masking and User Control:} Before any summary or capsule is exported, sensitive data fields (e.g., names, locations) are automatically detected and masked at the semantic level (e.g., replaced with tags such as \texttt{[PERSON NAME]} or \texttt{[LOCATION]}). Users may review or request additional masking as desired.

    \item \textbf{Memory Capsule Generation and Tagging:} Clusters periodically compress, encrypt, and tag key adaptation statistics, eligibility traces, and local context as “memory capsules.” Tagging leverages high-dimensional vector embeddings, context hashes, or interpretable semantic labels to enable rapid, context-driven search and retrieval.

    \item \textbf{Global Coordination, Storage, and Audit:} Memory capsules are uploaded to decentralized storage (e.g., IPFS, Filecoin) and indexed on a blockchain ledger with their semantic tags, cryptographic hashes, and access control metadata. The ledger records all capsule events (creation, update, borrowing, respawning) and enforces credit allocation and permissioning via smart contracts.

    \item \textbf{Rapid Memory Recall and Resilience:} When a device or cluster requires relevant long-term knowledge, it queries the blockchain-backed index for capsules with similar context tags. Only authorized nodes may decrypt and integrate this information, preserving both security and efficiency. Devices that drop out or rejoin can quickly resynchronize by retrieving the most recent, relevant capsules.
\end{itemize}

\paragraph{Summary:}
This architecture enables robust, privacy-preserving, and fully auditable distributed learning—combining local adaptation, elastic topology, secure memory management, and blockchain-based incentives for scalable, lifelong AI.


\section{Detailed Memory, Temporal Plasticity, and Network Growth}

\subsection{Hierarchical Memory Management}
Each threshold-gating node encodes activations and error signals in a local buffer, forming the basis of rapid adaptation. Upon each firing event (by threshold or timer), the node:
\begin{itemize}
    \item Appends an activation record to its local memory,
    \item Updates an eligibility trace reflecting recency and importance of activity,
    \item Periodically flushes or compresses these records into a “memory capsule” for cluster memory.
    \begin{figure}
        \centering
        \includegraphics[width=0.6\linewidth]{architecture_diagrams/37f16f0b-ac7a-44b5-8a94-e48c92587896.png}
        \caption{Memory and Cost Management}
        \label{fig:memory-and-cost-management}
    \end{figure}
\end{itemize}
Cluster memory on each device aggregates these capsules, enabling:
\begin{itemize}
    \item Local error evaluation and novelty detection,
    \item Coordination of adaptation rates and thresholds,
    \item Specialization and decision-making on node split/duplication.
\end{itemize}
At longer intervals, device-level 'consolidation' aggregates, compresses, and encrypts the important features and adaptation statistics of all clusters, producing a global 'memory capsule' for upload to distributed storage and blockchain registration.

\subsection{Temporal Functions and Plasticity}
Plasticity arises from the interplay of thresholds, timers, and eligibility traces:
\begin{itemize}
    \item The threshold \(\delta\) of each node and the timer \(T\) are adaptive, modulated by the error and the input history.
    \item The firing of the threshold increases \(\delta\), making the node less sensitive; the firing of the timer decreases \(\delta\), making it more sensitive.
    \item The timer \(T\) is lengthened or shortened according to firing patterns, allowing flexible 'pacing' and adaptation.
    \item Eligibility traces act as temporally decaying buffers, tagging events for future updates in response to global error or reward.
\end{itemize}
This enables both short-term and long-term dependencies, supports bursty or nonstationary input, and provides stable yet flexible specialization.

\subsection{Distributed Growth, Duplication, and Pruning}
The network structure evolves dynamically:
\begin{itemize}
    \item \textbf{Edge Duplication:} High-usage edges are duplicated to increase bandwidth and redundancy.
    \item \textbf{Node Splitting:} Overloaded nodes are split, with parameters and memory inherited and then diverged; split threshold grows with each division.
    \item \textbf{Edge/Node Pruning:} Underused edges are pruned; nodes without input are removed.
    \item \textbf{Cross-Device Expansion:} As new devices join, global memory enables rapid assignment of clusters/tasks and seamless scaling. Upon dropout, the device's last memory is preserved; the rejoining devices instantly resync.
\end{itemize}

\subsection{Coordinated Expansion via Blockchain and Incentives}
Smart contracts on blockchain:
\begin{itemize}
    \item Propose and audit structural changes (e.g., cluster migration, expansion, redundancy),
    \item Allocate incentives for hosting memory, supporting rebalancing, and network healing,
    \item Maintain trust, fairness, and transparency in all memory and adaptation events.
\end{itemize}

\section{Three-Level Memory and Global Coordination}
A memory hierarchy ensures robustness:
\begin{itemize}
    \item \textbf{Local Node Memory:} Accumulators, thresholds, timers, eligibility traces, and recent activation history.
    \item \textbf{Local Cluster Memory:} Aggregates outputs/errors, steers local adaptation.
    \item \textbf{Global Memory (Distributed/Cloud):} Consolidated capsules uploaded to redundant storage, indexed and auditable via blockchain.
\end{itemize}
The \textbf{Global Aggregator} periodically merges memory capsules, issues global modulatory signals, and enables fair credit assignment for all contributors.

\subsection{Device Dropout, Redundancy, and Rejoin}
Long-term memory is protected through:
\begin{itemize}
    \item Redundant, encrypted storage by archive nodes or decentralized services,
    \item Blockchain audit for every contribution,
    \item Seamless rejoin and resync for dropped devices using latest global memory.
\end{itemize}

\section{Blockchain, Auditability, and Incentives}
Blockchain provides:
\begin{itemize}
    \item Immutable records of memory uploads, structural changes, and device events,
    \item Transparent credit and incentive distribution via smart contracts,
    \item Proof-of-contribution and fair recognition for all participants,
    \item Guaranteed privacy (only hashes, encrypted models published; raw data never leaves device).
\end{itemize}

\section{Workflow and Experiments}
\begin{enumerate}
    \item The device performs local, forward-only learning with threshold-gating nodes, updating node, and cluster memory.
    \item At intervals, compresses and encrypts 'memory capsule', uploads to distributed storage, records hash/pointer on blockchain.
    \item The global aggregator consolidates capsules, updates global memory, and issues global modulatory signal and pricing/incentive feedback.
    \item The network grows or shrinks via edge/node split/duplication/pruning, coordinated by smart contracts.
    \item Devices dropping out have contributions persisted; rejoining is seamless via global memory resync.
\end{enumerate}
Experiments on multimodal data demonstrate: 
\begin{itemize}
    \item Fast adaptation to new patterns and tasks,
    \item Robustness to device dropout,
    \item Efficient use of incentives for active contribution,
    \item Stable memory and dynamic network growth.
\end{itemize}

\section{Discussion}
\subsection{Advantages}
\begin{itemize}
    \item \textbf{Biological Plausibility:} Mirrors known brain mechanisms—local adaptation, temporal memory, eligibility traces, and global modulation.
    \item \textbf{Privacy-First and Decentralized:} Raw data never leaves user device; only encrypted summaries are shared.
    \item \textbf{Resilience and Scaling:} Distributed, auditable memory and network growth allow for robust, self-healing AI at scale.
    \item \textbf{Community-Driven Incentives:} Smart contracts ensure fair compensation for all contributors and archival nodes.
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item \textbf{Communication/Storage Overhead:} Memory capsules and blockchain records, while small, must be managed efficiently at scale.
    \item \textbf{Parameter Tuning:} Network plasticity and incentive models require continued research for optimal convergence and fairness.
    \item \textbf{Infrastructure Dependencies:} Distributed storage and blockchain security depend on community and ecosystem support.
\end{itemize}

\section{Conclusion and Future Work}
We have introduced a distributed, modular, forward-only neural network architecture inspired by biological memory, with dynamic adaptation, blockchain-audited memory, and community-driven incentives. Our system provides a blueprint for privacy-first, resilient, and scalable AI learning. Future research will focus on larger-scale deployments, automated parameter tuning, neuromorphic implementation, and formal theoretical analysis.

\bibliographystyle{plain}
\begin{thebibliography}{99}
\bibitem{Lisman2020} Lisman, J. (2020). Memory formation and error signals in hippocampus. \emph{Trends in Cognitive Sciences}, 24(8), 678-689.
\bibitem{Sara2009} Sara, S. J. (2009). The locus coeruleus and noradrenergic modulation of memory. \emph{Nature Reviews Neuroscience}, 10(3), 211-223.
\bibitem{Doya2000} Doya, K. (2000). Complementary roles of basal ganglia and cerebellum in learning and motor control. \emph{Current Opinion in Neurobiology}, 10(6), 732-739.
\bibitem{Gerstner2018} Gerstner, W., Lehmann, M., Liakoni, V., Corneil, D., Brea, J. (2018). Eligibility traces and plasticity on behavioral time scales: Experimental support of neohebbian three-factor learning rules. \emph{Frontiers in Neural Circuits}, 12, 53.
\bibitem{Yagishita2014} Yagishita, S., et al. (2014). A critical time window for dopamine actions on the structural plasticity of dendritic spines. \emph{Science}, 345(6204), 1616-1620.
\bibitem{Pozzorini2013} Pozzorini, C., Naud, R., Mensi, S.,Gerstner, W. (2013). Temporal whitening by power-law adaptation in neocortical neurons. \emph{Nature Neuroscience}, 16(7), 942-948.
\bibitem{Fusi2005} Fusi, S., Drew, P. J.,Abbott, L. F. (2005). Cascade models of synaptically stored memories. \emph{Neuron}, 45(4), 599-611.
\bibitem{Holtmaat2009} Holtmaat, A.,Svoboda, K. (2009). Experience-dependent structural synaptic plasticity in the mammalian brain. \emph{Nature Reviews Neuroscience}, 10(9), 647-658.
\bibitem{Perin2011} Perin, R., Berger, T. K.,Markram, H. (2011). A synaptic organizing principle for cortical neuronal groups. \emph{Proceedings of the National Academy of Sciences}, 108(13), 5419-5424.
% Add further references for blockchain, distributed AI, or experiments as needed.
\bibitem{Rumelhart1986}
D.E.~Rumelhart, G.E.~Hinton, and R.J.~Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323(6088):533--536, 1986.

\bibitem{LeCun2015}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521(7553):436--444, 2015.

\bibitem{Izhikevich2004}
E.M.~Izhikevich.
\newblock Which model to use for cortical spiking neurons?
\newblock \emph{IEEE Transactions on Neural Networks}, 15(5):1063--1070, 2004.

\bibitem{Bengio2015}
Y.~Bengio, A.~Courville, and P.~Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 35(8):1798--1828, 2013.

\bibitem{Hinton2022}
G.E.~Hinton.
\newblock Forward-forward algorithm: Some preliminary investigations.
\newblock \emph{arXiv preprint arXiv:2212.13345}, 2022.

\bibitem{Hebb1949}
D.O.~Hebb.
\newblock \emph{The Organization of Behavior: A Neuropsychological Theory}.
\newblock Wiley, 1949.

\bibitem{Ngiam2011}
J.~Ngiam, A.~Khosla, M.~Kim, J.~Nam, H.~Lee, and A.Y.~Ng.
\newblock Multimodal deep learning.
\newblock In \emph{Proceedings of the 28th Intl. Conf. on Machine Learning (ICML)}, 2011.

\bibitem{Ramachandram2017}
D.~Ramachandram and D.~Rodr\'{i}guez-Serrano.
\newblock Deep multimodal learning: A survey on recent advances and trends.
\newblock \emph{IEEE Signal Processing Magazine}, 34(6):96--108, 2017.

\bibitem{Gerstner2018}
W.~Gerstner, M.~Lehmann, V.~Liakoni, D.~Corneil, and J.~Brea.
\newblock Eligibility traces and plasticity on behavioral time scales: Experimental support of neohebbian three-factor learning rules.
\newblock \emph{Frontiers in Neural Circuits}, 12:53, 2018.

\bibitem{Pozzorini2013}
C.~Pozzorini, R.~Naud, S.~Mensi, and W.~Gerstner.
\newblock Temporal whitening by power-law adaptation in neocortical neurons.
\newblock \emph{Nature Neuroscience}, 16(7):942--948, 2013.

\bibitem{Fusi2005}
S.~Fusi, P.J.~Drew, and L.F.~Abbott.
\newblock Cascade models of synaptically stored memories.
\newblock \emph{Neuron}, 45(4):599--611, 2005.

\bibitem{Lisman2020}
J.~Lisman.
\newblock Memory formation and error signals in hippocampus.
\newblock \emph{Trends in Cognitive Sciences}, 24(8):678--689, 2020.

\bibitem{Sara2009}
S.J.~Sara.
\newblock The locus coeruleus and noradrenergic modulation of memory.
\newblock \emph{Nature Reviews Neuroscience}, 10(3):211--223, 2009.

\bibitem{Kairouz2021}
P.~Kairouz, H.B.~McMahan, B.~Avent, et al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends in Machine Learning}, 14(1--2):1--210, 2021.

\bibitem{Christidis2016}
K.~Christidis and M.~Devetsikiotis.
\newblock Blockchains and smart contracts for the internet of things.
\newblock \emph{IEEE Access}, 4:2292--2303, 2016.

\bibitem{Zyskind2015}
G.~Zyskind, O.~Nathan, and A.~Pentland.
\newblock Decentralizing privacy: Using blockchain to protect personal data.
\newblock In \emph{2015 IEEE Security and Privacy Workshops}, pages 180--184, 2015.

\bibitem{Gerstner2018}
W.~Gerstner, M.~Lehmann, V.~Liakoni, D.~Corneil, and J.~Brea.
\newblock Eligibility traces and plasticity on behavioral time scales: Experimental support of neohebbian three-factor learning rules.
\newblock \emph{Frontiers in Neural Circuits}, 12:53, 2018.

\bibitem{Frey1997}
U.~Frey and R.G.M.~Morris.
\newblock Synaptic tagging and long-term potentiation.
\newblock \emph{Nature}, 385(6616):533--536, 1997.

\bibitem{Petrosyan2023}
Petrosyan, S., King, D., Rohit, S.
\newblock Decentralized Vector Databases and Blockchain-Indexed Storage for Large-Scale AI.
\newblock \emph{Proceedings of the International Conference on Distributed Systems and Blockchain}, 2023.

\bibitem{Buzsaki2019}
Buzsáki, G., Tingley, D.
\newblock Space and Time: The Hippocampus as a Sequence Generator.
\newblock \emph{Trends in Cognitive Sciences}, 23(10), 853-869, 2019.

\bibitem{Shohamy2008}
Shohamy, D., Adcock, R. A.
\newblock Dopamine and adaptive memory.
\newblock \emph{Trends in Cognitive Sciences}, 14(10), 464-472, 2010.

\bibitem{Frémaux2016}
Frémaux, N., Gerstner, W.
\newblock Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules.
\newblock \emph{Frontiers in Neural Circuits}, 9, 85, 2016.

\bibitem{Turrigiano2012}
Turrigiano, G.
\newblock Homeostatic synaptic plasticity: local and global mechanisms for stabilizing neuronal function.
\newblock \emph{Cold Spring Harbor Perspectives in Biology}, 4(1), a005736, 2012.

\bibitem{Sporns2016}
Sporns, O.
\newblock Modular structure and dynamics of brain networks.
\newblock \emph{Nature Reviews Neuroscience}, 17(10), 662–672, 2016.

\bibitem{Carr2011}
Carr, M. F., Jadhav, S. P., Frank, L. M.
\newblock Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval.
\newblock \emph{Nature Neuroscience}, 14(2), 147–153, 2011.

\bibitem{Girardeau2014}
Girardeau, G., Zugaro, M.
\newblock Hippocampal ripples and memory consolidation.
\newblock \emph{Current Opinion in Neurobiology}, 21(3), 452–459, 2011.

\bibitem{McClelland1995}
McClelland, J. L., McNaughton, B. L., O'Reilly, R. C.
\newblock Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.
\newblock \emph{Psychological Review}, 102(3), 419–457, 1995.

\end{thebibliography}

\end{document}
